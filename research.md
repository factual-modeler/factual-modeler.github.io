## Method Overview

摘要：端到端的视觉–语言–动作（vision–language–action, VLA）策略近期在多个场景中展现出令人印象深刻的效果，但如果把它们当作机器人的唯一大脑，在本质上仍存在明显局限。作为纯粹的“反应式”控制器，VLA 缺乏对动力学与因果结构的显式建模，这使得它在长时序任务上非常脆弱、难以诊断，尤其在分布偏移下极不安全——微小的感知误差就可能在轨迹中被不断放大，最终导致大幅偏移甚至失败。VLA 也无法系统性地“想象”未来、比较不同的动作序列，或在执行前过滤掉高风险的计划。

为克服这些限制，我们提出了一个“三位一体”的 VLA + 世界模型 + 因果智能 架构：在 VLA 主干之上，引入显式的具身世界模型和隐空间规划器。VLA 仍然通过以物体为中心的表征和语言引导的任务向量，提供语义感知与任务条件；但所有长时序的规划、想象与反事实推理，都在由世界模型学习到的紧凑隐变量状态空间中完成。在真正下发到硬件之前，智能体会在想象空间中展开多条候选动作序列，评估其预测结果（包括奖励和安全信号），并选择其中最有前景且风险最低的一条去执行。

这种将“看见并理解”（VLA）与“想象并决策”（世界模型 + 因果推理）相分离的方式，显著提升了任务成功率与样本效率，并大幅减少了不安全行为。我们在多种任务上的实验证明，超越纯端到端 VLA、对具身世界的因果过程进行显式建模，是构建可复用的通用机器人大脑的关键一步。

![P1](./img/md-p1.jpg)



## Training

### 要实现通用机器人大脑，必须同时解决四个系统级难题
- **新一代架构:**  **Beyond end-to-end** 显式世界模型 + 因果推理
- **高效数据引擎:** 多本体统一log schema + 针对性采集因果、物理属性数据
- **持续学习闭环:** 部署中的失败自动回流，支持“用提示/演示”快速适配
- **统一系统接口:** **标准API** robot ↔ brain ↔ cloud，安全 & 可回滚




### VLA + World Model + Causal Intelligence 的通用具身大脑
**我们的架构设计原则：**
- 可跨机器人本体复用：模块化建模、状态表示与机型解耦
- 可解释 & 可控制：结构化表示世界、显式建模因果和物理规律
- 高效 & 安全：在 latent 里想象和规划，只解码少数关键步骤，减少真机风险

![P6](./img/md-p6.png)



### 为什么有效： 懂因果的世界模型
在多种机器人操控、 locomotion benchmark、和long horizon任务 上，相比传统世界模型，我们的方法：
- **成功率提升 25-50%**
- **所需样本减少 5-10x**
- 在跨任务 / 跨环境 / 跨奖励函数时，**仍保持稳定泛化**
![P2](img/md-p2.png)
![P3](img/md-p3.png)
![P4](img/md-p4.png)





### 高效数据引擎
![P5](img/md-p5.png)




## 想象
给定同一个视觉–语言指令，VLA 先产生多组候选 action 序列，再在 world model 中进行"想象"rollout，评估每一组的效果，从中挑选最优的一组再下发给真实机器人执行。由于 VLA 基于 imitation learning，对环境中的微小变化非常敏感，很容易导致轨迹偏移；通过在“想象空间”里先跑一遍、再真正执行，我们可以显著提高鲁棒性。我们会对比 加入 world model 规划 和 直接用 VLA 执行 两种方案的表现，这也是我们框架的一个核心卖点。

## 泛化能力
在 world model 上做一些泛化实验，展示其在未见场景/任务上的预测与规划能力，从而突出我们方案的泛化优势